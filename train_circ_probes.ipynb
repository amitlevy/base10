{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from circular_probe import train_circular_probe\n",
    "import numpy\n",
    "from tqdm import tqdm\n",
    "from general_ps_utils import ModelAndTokenizer\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "params = {\n",
    "    'model_name': \"meta-llama/Meta-Llama-3-8B\", #\"mistralai/Mistral-7B-v0.1\",\n",
    "    'use_4bit': False,\n",
    "    'epochs': 10_000,\n",
    "    'lr': 0.0005,\n",
    "    'numbers': 2000,\n",
    "    'batch_size': 2000,\n",
    "    'exclude': 'random', # numbers to exclude from training set\n",
    "    'exclude_count': 200,\n",
    "    'positions': 1,\n",
    "    'shuffle': True,\n",
    "    'bases': [10,11],\n",
    "    'start_layer': 0,\n",
    "    'bias': False\n",
    "}\n",
    "\n",
    "\n",
    "print(f\"Params:\\n\\n{params}\")\n",
    "\n",
    "if params['exclude'] == 'random':\n",
    "    params['exclude'] = numpy.random.choice(params['numbers'], params['exclude_count'], replace=False)\n",
    "\n",
    "mt = ModelAndTokenizer(\n",
    "    model_name=params['model_name'],\n",
    "    use_4bit=params['use_4bit'],\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "tokenizer = mt.tokenizer\n",
    "num_to_hidden = dict()\n",
    "\n",
    "# move device to cuda because for some reason it is not\n",
    "mt.model.to('cuda')\n",
    "\n",
    "print(f\"device of model is {mt.model.device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in tqdm(range(params['numbers']), delay=120):\n",
    "    text_for_embeddings = \"\"\n",
    "\n",
    "    for _ in range(params['positions']):\n",
    "        text_for_embeddings += str(i) + \" \"\n",
    "    text_for_embeddings = text_for_embeddings[:-1]\n",
    "\n",
    "    x = tokenizer.encode(text_for_embeddings, return_tensors='pt')\n",
    "    x = x.to(mt.device)\n",
    "    hidden_states = mt.model(x, output_hidden_states=True).hidden_states\n",
    "\n",
    "    num_to_hidden[i] = hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# need to average over all layers per basis\n",
    "import pandas as pd\n",
    "\n",
    "# set up df\n",
    "df = pd.DataFrame(columns=['layer', 'basis', 'accuracy'])\n",
    "\n",
    "\n",
    "basis_acc = dict()\n",
    "basis_acc_max = dict()\n",
    "basis_acc_argmax = dict()\n",
    "for basis in tqdm(params['bases']):\n",
    "    print(f\"Training cyclic probe on basis: {basis}\")\n",
    "    params['basis'] = basis\n",
    "    layer_acc = []\n",
    "    for layer in range(params['start_layer'], mt.num_layers):\n",
    "        params['layers'] = [layer]\n",
    "        acc, circular_probe = train_cyclic_probe(params, mt, num_to_hidden)\n",
    "        print(f\"Layer: {layer}, Accuracy: {acc}\")\n",
    "        layer_acc.append(acc)\n",
    "\n",
    "        # add to df\n",
    "        df_delta = pd.DataFrame([[layer, basis, acc]], columns=['layer', 'basis', 'accuracy'])\n",
    "        df = pd.concat([df, df_delta], ignore_index=True)\n",
    "\n",
    "    basis_acc[basis] = numpy.mean(layer_acc)\n",
    "    basis_acc_max[basis] = numpy.max(layer_acc)\n",
    "    basis_acc_argmax[basis] = numpy.argmax(layer_acc)\n",
    "\n",
    "# round all acc numbers to 3 decimal places\n",
    "basis_acc = {k: round(v, 3) for k, v in basis_acc.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Model: {params['model_name']}\")\n",
    "print(f\"Number of numbers: {params['numbers']}\")\n",
    "print(f\"Epochs: {params['epochs']}\")\n",
    "print(f\"Start layer: {params['start_layer']}\")\n",
    "\n",
    "# print results as a table\n",
    "print(\"Basis\\tAccuracy\")\n",
    "\n",
    "for basis, acc in basis_acc.items():\n",
    "    print(f\"{basis}\\t{acc}\")\n",
    "\n",
    "for basis, acc in basis_acc_max.items():\n",
    "    print(f\"{basis}\\t{acc} (layer: {basis_acc_argmax[basis]})\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
